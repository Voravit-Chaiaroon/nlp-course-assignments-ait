{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.2'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: The fool wanders, the wise man travels.\n",
      "Thai: คนโง่พเนจร คนฉลาดท่องเที่ยว\n",
      "English: One of these days is none of these days.\n",
      "Thai: หนึ่งในวันเหล่านี้คือไม่มีวันเหล่านี้เลย\n",
      "English: Necessity is a hard nurse, but she raises strong children.\n",
      "Thai: ความจำเป็นเป็นสิ่งที่ยาก, แต่เธอสามารถเลี้ยงลูกที่เข็งแรงได้\n",
      "English: In one ear and out the other.\n",
      "Thai: เข้าหู้ข้างหนึ่งและออกอีกข้างหนึ่ง\n",
      "English: It can't happen here is number one on the list of famous last words.\n",
      "Thai: ไม่สามารถเกิดขึ้นที่นี่ได้คือคำพูดสุดท้ายที่ดังบนลิสต์\n",
      "English: Facts do not cease to exist because they are ignored.\n",
      "Thai: ความจริงไม่ได้หายไปเพราะถูกเมิน\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchdata.datapipes.iter import FileLister, FileOpener, ShardingFilter\n",
    "from pythainlp.util import normalize\n",
    "\n",
    "# Step 1: List dataset CSV files\n",
    "datapipe = FileLister(root=\"datasets/scb-mt-en-th-2020/\", recursive=True)\n",
    "\n",
    "# Step 2: Open files in binary mode\n",
    "datapipe = FileOpener(datapipe, mode=\"rb\")\n",
    "\n",
    "# Step 3: Apply sharding filter\n",
    "sharded_dp = ShardingFilter(datapipe)\n",
    "\n",
    "# train = sharded_dp\n",
    "\n",
    "# Step 4: Read and yield data\n",
    "def process_csv(file_tuple):\n",
    "    file_path, file_obj = file_tuple  # Extract file path and file stream\n",
    "    df = pd.read_csv(file_obj)  # Read CSV into DataFrame\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        en_text = row[\"en_text\"]\n",
    "        th_text = normalize(row[\"th_text\"]) # Normalize Thai text using pythainlp\n",
    "        yield en_text, th_text\n",
    "\n",
    "# Step 5: Apply processing function to each file\n",
    "train = datapipe.flatmap(process_csv)\n",
    "\n",
    "# Step 6: Iterate and debug output\n",
    "for i, (en_tokens, th_tokens) in enumerate(train):\n",
    "    print(f\"English: {en_tokens}\")\n",
    "    print(f\"Thai: {th_tokens}\")\n",
    "    \n",
    "    if i == 5:  # Limit output to first 5 examples\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The fool wanders, the wise man travels.', 'คนโง่พเนจร คนฉลาดท่องเที่ยว')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44168"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train.random_split(total_length=train_size, weights = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30917"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8834"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4417"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'th'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import word_tokenize  # Thai tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = lambda text: word_tokenize(text, keep_whitespace=False)  # Thai with PyThaiNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[134, 14, 8, 0, 8]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweet'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16902"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[TRG_LANGUAGE](['eos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab_transform, 'vocab_transform.pth')\n",
    "#Save the vocab_transform to be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/note3/Desktop/Python-fo-Natural-Language-Processing-main/venv/lib/python3.9/site-packages/torch/utils/data/graph_settings.py:103: UserWarning: `shuffle=True` was set, but the datapipe does not contain a `Shuffler`. Adding one at the end. Be aware that the default buffer size might not be sufficient for your task.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, de in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 22])\n",
      "German shape:  torch.Size([64, 27])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"German shape: \", de.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device, attention_type):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device, attention_type)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, attention_type, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device, attention_type)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device, attention_type):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        self.attention_type = attention_type\n",
    "                \n",
    "        # Initialize W for Multiplicative Attention\n",
    "        if self.attention_type == \"multiplicative\":\n",
    "            self.W = nn.Parameter(torch.Tensor(self.head_dim, self.head_dim))\n",
    "            nn.init.xavier_uniform_(self.W)  # Xavier initialization for better convergence\n",
    "            \n",
    "        if self.attention_type == \"additive\":\n",
    "            self.W1 = nn.Parameter(torch.Tensor(self.head_dim, self.head_dim))\n",
    "            self.W2 = nn.Parameter(torch.Tensor(self.head_dim, self.head_dim))\n",
    "            self.v = nn.Parameter(torch.Tensor(self.head_dim, 1))  # v for scoring\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "\n",
    "        #From attention_type, we can choose which one to use\n",
    "        if self.attention_type == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "            if mask is not None:\n",
    "                energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            attention = energy\n",
    "            x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        elif self.attention_type == \"multiplicative\":\n",
    "            energy = torch.matmul(torch.matmul(Q, self.W), K.permute(0, 1, 3, 2))\n",
    "            if mask is not None:\n",
    "                energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            attention = energy\n",
    "            x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        elif self.attention_type == \"additive\":\n",
    "            Q_transformed = torch.matmul(Q, self.W2)  # [batch_size, n_heads, key_len, head_dim]\n",
    "            K_transformed = torch.matmul(K, self.W1)  # [batch_size, n_heads, query_len, head_dim]\n",
    "            # Expand dimensions for broadcasting    \n",
    "            Q_transformed = Q_transformed.unsqueeze(3)  # [batch_size, n_heads, query_len, 1, head_dim]\n",
    "            K_transformed = K_transformed.unsqueeze(2)  # [batch_size, n_heads, 1, key_len, head_dim]\n",
    "            combined = torch.tanh(Q_transformed + K_transformed)\n",
    "            energy = torch.matmul(combined, self.v).squeeze(-1) # [batch_size, n_heads, query_len, key_len]\n",
    "            if mask is not None:\n",
    "                energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            attention = energy\n",
    "            x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        else : #self.attention_type == \"scaled_dot\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "            if mask is not None:\n",
    "                energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            attention = torch.softmax(energy, dim = -1)\n",
    "            x = torch.matmul(self.dropout(attention), V)\n",
    "  \n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device, attention_type):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device, attention_type)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device, attention_type)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, attention_type, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device, attention_type)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16902, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(12786, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=12786, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attention_type = 'multiplicative'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4326912\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "3273216\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "3273216\n",
      " 12786\n",
      "______\n",
      "14900210\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 16s\n",
      "\tTrain Loss: 6.787 | Train PPL: 886.572\n",
      "\t Val. Loss: 7.021 |  Val. PPL: 1119.687\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Attention Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16902, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(12786, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=12786, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attention_type = 'general'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "model_general_attention = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model_general_attention.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 8s\n",
      "\tTrain Loss: 9.478 | Train PPL: 13068.672\n",
      "\t Val. Loss: 9.485 |  Val. PPL: 13161.738\n",
      "Epoch: 02 | Time: 2m 10s\n",
      "\tTrain Loss: 9.478 | Train PPL: 13071.113\n",
      "\t Val. Loss: 9.485 |  Val. PPL: 13161.738\n",
      "Epoch: 03 | Time: 2m 11s\n",
      "\tTrain Loss: 9.478 | Train PPL: 13068.106\n",
      "\t Val. Loss: 9.485 |  Val. PPL: 13161.738\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 3\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model_general_attention.__class__.__name__}{attention_type}Attention.pt'\n",
    "\n",
    "train_losses_general = []\n",
    "valid_losses_general = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_general_attention, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_general_attention, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_general_attention.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicative Attention Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16902, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(12786, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=12786, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attention_type = 'multiplicative'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "model_multiplicative_attention = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model_multiplicative_attention.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 16s\n",
      "\tTrain Loss: 9.474 | Train PPL: 13018.914\n",
      "\t Val. Loss: 9.468 |  Val. PPL: 12942.913\n",
      "Epoch: 02 | Time: 2m 17s\n",
      "\tTrain Loss: 9.474 | Train PPL: 13019.833\n",
      "\t Val. Loss: 9.468 |  Val. PPL: 12942.913\n",
      "Epoch: 03 | Time: 2m 13s\n",
      "\tTrain Loss: 9.474 | Train PPL: 13022.171\n",
      "\t Val. Loss: 9.468 |  Val. PPL: 12942.913\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 3\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model_multiplicative_attention.__class__.__name__}{attention_type}Attention.pt'\n",
    "\n",
    "train_losses_multiplicative = []\n",
    "valid_losses_multiplicative = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_multiplicative_attention, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_multiplicative_attention, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_multiplicative_attention.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Attention Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16902, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(12786, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=12786, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attention_type = 'additive'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device,\n",
    "              attention_type)\n",
    "\n",
    "model_additive_attention = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model_additive_attention.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 3m 28s\n",
      "\tTrain Loss: nan | Train PPL:     nan\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n",
      "Epoch: 02 | Time: 3m 29s\n",
      "\tTrain Loss: nan | Train PPL:     nan\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n",
      "Epoch: 03 | Time: 3m 55s\n",
      "\tTrain Loss: nan | Train PPL:     nan\n",
      "\t Val. Loss: nan |  Val. PPL:     nan\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 3\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model_additive_attention.__class__.__name__}{attention_type}Attention.pt'\n",
    "\n",
    "train_losses_additive = []\n",
    "valid_losses_additive = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_additive_attention, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_additive_attention, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_additive_attention.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better\n",
    "    #fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14f6efdc0>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses_general, label = 'train gen loss')\n",
    "ax.plot(valid_losses_general, label = 'valid gen loss')\n",
    "ax.plot(train_losses_multiplicative, label = 'train mul loss')\n",
    "ax.plot(valid_losses_multiplicative, label = 'valid mul loss')\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/Seq2SeqTransformeradditiveAttention.pt'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 9.483 | Test PPL: 13138.906 |\n",
      "| Test Loss: 9.469 | Test PPL: 12946.385 |\n"
     ]
    }
   ],
   "source": [
    "model_general_attention.load_state_dict(torch.load('models/Seq2SeqTransformergeneralAttention.pt'))\n",
    "model_multiplicative_attention.load_state_dict(torch.load('models/Seq2SeqTransformermultiplicativeAttention.pt'))\n",
    "# model_additive_attention.load_state_dict(torch.load('models/Seq2SeqTransformeradditiveAttention.pt'))\n",
    "test_loss_general = evaluate(model_general_attention, test_loader, criterion, test_loader_length)\n",
    "test_loss_multiplicative = evaluate(model_multiplicative_attention, test_loader, criterion, test_loader_length)\n",
    "# test_loss_additive = evaluate(model_additive_attention, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss_general:.3f} | Test PPL: {math.exp(test_loss_general):7.3f} |')\n",
    "print(f'| Test Loss: {test_loss_multiplicative:.3f} | Test PPL: {math.exp(test_loss_multiplicative):7.3f} |')\n",
    "# print(f'| Test Loss: {test_loss_additive:.3f} | Test PPL: {math.exp(test_loss_additive):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(iter(test))\n",
    "srcTest = \"Hello, how are you?\"\n",
    "trgTest = \"สวัสดี สบายดีไหม\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fool wanders, the wise man travels.'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'คนโง่พเนจร คนฉลาดท่องเที่ยว'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 3853,    5,  136,   32,   16,   19,    3])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text = text_transform[SRC_LANGUAGE](srcTest).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 1963, 3634,  109,    3])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text = text_transform[TRG_LANGUAGE](trgTest).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model_general_attention(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 12786])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12786])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12786])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3006, 3006, 3006, 1433])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เกษียณอายุ\n",
      "เกษียณอายุ\n",
      "เกษียณอายุ\n",
      "ผิดกฎหมาย\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 8])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'Hello', ',', 'how', 'are', 'you', '?', '<eos>']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](srcTest) + ['<eos>']\n",
    "# src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'เกษียณอายุ', 'เกษียณอายุ', 'เกษียณอายุ', 'ผิดกฎหมาย']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max] \n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'เกษียณอายุ เกษียณอายุ เกษียณอายุ'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove the <sos> token and join the tokens into a sentence\n",
    "translated_sentence = \" \".join(trg_tokens[1:-1])  # Exclude <sos> and <eos>\n",
    "translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "\n",
    "    # Load a Thai font\n",
    "    thai_font_path = \"/System/Library/Fonts/Supplemental/Ayuthaya.ttf\"  # Update with the correct path\n",
    "    thai_font = fm.FontProperties(fname=thai_font_path, size=12)\n",
    "\n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45, fontproperties=thai_font)\n",
    "    ax.set_yticklabels(y_ticks, fontproperties=thai_font)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](srcTest) + ['<eos>']\n",
    "# src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/70m37kyn5cl881qwq4g7m42h0000gn/T/ipykernel_18258/537641849.py:23: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45, fontproperties=thai_font)\n",
      "/var/folders/jm/70m37kyn5cl881qwq4g7m42h0000gn/T/ipykernel_18258/537641849.py:24: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks, fontproperties=thai_font)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Attentions               |   Training Loss |   Training PPL |   Validation Loss |   Validation PPL |\n",
      "|--------------------------+-----------------+----------------+-------------------+------------------|\n",
      "| General Attention        |           9.473 |        13068.1 |             9.485 |          13161.7 |\n",
      "| Multiplicative Attention |           9.503 |        13018.9 |             9.468 |          12942.9 |\n",
      "| Additive Attention       |         nan     |          nan   |           nan     |            nan   |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate([['General Attention',  9.473, 13068.106, 9.485, 13161.738], \n",
    "                ['Multiplicative Attention',  9.503, 13018.914, 9.468, 12942.913],\n",
    "                ['Additive Attention',  'nan' ,'nan', 'nan', 'nan']],\n",
    "                headers=[\"Attentions\",\"Training Loss\",\"Training PPL\",\"Validation Loss\",'Validation PPL'], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
